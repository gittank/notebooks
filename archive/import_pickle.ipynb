{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "import sys\n",
    "from os import listdir\n",
    "\n",
    "def tableName(run='004'):\n",
    "    tbName0 = 'rct007_testID'\n",
    "    tbName1 = run+'_flowindata'\n",
    "    tbName2 = run+'_flowoutdata'\n",
    "    tbName3 = run+'_plcdata'\n",
    "    tbName4 = run+'_voltagedata'\n",
    "    tbName5 = run+'_logdata'\n",
    "    return [tbName0, tbName1, tbName2, tbName3, tbName4, tbName5]\n",
    "\n",
    "def parseLog(cursor, run):\n",
    "    # read log file\n",
    "    log = readNoDateTable(cursor, run+'_logdata')\n",
    "    \n",
    "    # get the start and stop states by looking at state variable\n",
    "    stateStart = log[log.state=='1'].date.reset_index(drop=True)\n",
    "    stateEnd = log[log.state=='0'].date.reset_index(drop=True) \n",
    "    \n",
    "    # check to ensure that we have the same number of states\n",
    "    lenStart = len(stateStart)\n",
    "    lenEnd = len(stateEnd)\n",
    "    if lenStart != lenEnd:\n",
    "        stop = min(lenStart, lenEnd)\n",
    "        stateStart = stateStart[0:stop]\n",
    "        stateEnd = stateEnd[0:stop]\n",
    "        print 'States have been truncated start:%d end:%d'%(lenStart, lenEnd)\n",
    "    \n",
    "    return stateStart, stateEnd\n",
    "\n",
    "def getEventIdOffset(dbase, path):\n",
    "    log = pd.read_pickle(path+'/'+dbase+'_log.pkl')\n",
    "    return log.iloc[-1,-2]\n",
    "\n",
    "def pickleTable(df, path, dbase, run, eventID):\n",
    "    # get rid of date\n",
    "    df.drop('date', axis=1, inplace=True)\n",
    "    \n",
    "    #all the files to read and save\n",
    "    for col in df.columns:\n",
    "        filename = path+dbase+'_'+run+'_'+col+'_'+str(eventID)+'.pkl'\n",
    "        if len(df[col]) !=0:\n",
    "            print 'Pickling %s \\n'%filename\n",
    "            pd.DataFrame(df[col]).to_pickle(filename)\n",
    "        else:\n",
    "            print 'Event: %d Run: %s Sensor: %s is null'%(eventId, run, col)\n",
    "\n",
    "def pickleEvent(dbase, run, start, end, event, path):\n",
    "    cursor = connectDbase(dbase)\n",
    "\n",
    "    # get table names, remove non data tables\n",
    "    tbNameList = tableName(run)[1:-1]\n",
    "    if dbase == 'rct008':\n",
    "        tbNameList = tableName(run)[3:-1]\n",
    "\n",
    "    for tbName in tbNameList:\n",
    "        # read table\n",
    "        dfTable = readDateTable(cursor, tbName, start, end)\n",
    "        \n",
    "        # if it is not empty make a pickel file for each sensor\n",
    "        if len(dfTable) != 0:\n",
    "            pickleTable(dfTable, path, dbase, run, event)\n",
    "        else:\n",
    "            #if it's empty print this an keep going\n",
    "            print 'Table: %s is empty.\\n'%tbName\n",
    "\n",
    "    \n",
    "def pickleSensorFiles(cursor, dbase, run, path, forceWrite = False):\n",
    "    \n",
    "    log = pd.read_pickle(path+dbase+'_log.pkl')\n",
    "    log = log[log['run']==run]\n",
    "    \n",
    "    # find the last event id stored to pickle\n",
    "    dirList = os.listdir(path)\n",
    "    pklFileList = [s for s in dirList if 'pkl' in s and dbase+'_'+run in s and not '_log.pkl' in s]\n",
    "    \n",
    "    temp = [x.split('_')[-1] for x in pklFileList] \n",
    "    storedEvent = 0\n",
    "    if temp:\n",
    "        storedEvent = max([int(x.split('.')[0]) for x in temp])\n",
    "        \n",
    "    for eventId in log.eventId.unique():\n",
    "        if forceWrite or (eventId>storedEvent):\n",
    "            s = str(log[(log.eventId==eventId) & (log.state =='1')].date.values[0])    \n",
    "            e = str(log[(log.eventId==eventId) & (log.state =='0')].date.values[0])    \n",
    "            pickleEvent(dbase, run, s, e, eventId, path)\n",
    "\n",
    "            \n",
    "def logFileDataFrame(cursor, dbase, run, path):  \n",
    "    \n",
    "    firstRun = 4\n",
    "    offset = 0\n",
    "    \n",
    "    if int(run) > firstRun:\n",
    "        offset = getEventIdOffset(dbase, path) + 1\n",
    "        \n",
    "    # read log file\n",
    "    log = readNoDateTable(cursor, run+'_logdata')\n",
    "    \n",
    "    if run == '004' and dbase == 'rct007':\n",
    "        print 'make corrections'\n",
    "        log.iloc[231,1] = 3\n",
    "        log.iloc[288,1] = 2\n",
    "\n",
    "    # error check and format log file\n",
    "    # keep only states 1 and 0\n",
    "    log = log[(log['state'] == '1') | (log['state'] == '0')].reset_index(drop=True)\n",
    "    \n",
    "    # if the comment contains the word 'error' get rid of that row\n",
    "    idx  = [c[0:15] == 'Data collection' for c in log.comment.values]\n",
    "    log = log[idx].reset_index(drop=True)\n",
    "\n",
    "    # remove repeated states\n",
    "    # here we keep the first state remove the rest that repeat\n",
    "    states = log.state.apply(int)\n",
    "    dState = states.diff()\n",
    "    dState[0] = 1\n",
    "    log = log[(dState == 1) | (dState == -1)].reset_index(drop=True)\n",
    "\n",
    "    # if last state is not STOP, then get rid of it\n",
    "    lastState = int(log.iloc[-1,:]['state'])\n",
    "    \n",
    "    if lastState == 1:\n",
    "        log = log.iloc[0:-1,:].reset_index(drop=True)\n",
    "\n",
    "    # check time stamps.\n",
    "    if log.date.diff().dt.seconds.any() < 0:\n",
    "         sys.exit(\"Non-Increasing Sequential Time in events\")\n",
    "\n",
    "    # add event ids and run\n",
    "    eventId = np.arange(len(log[log.state == '1'])).repeat(2)\n",
    "    log['eventId'] = eventId + offset\n",
    "    log['run'] = run\n",
    "\n",
    "    return log\n",
    "    \n",
    "def pickleLogFile(cursor, dbase, runList, path, forceWrite = False):\n",
    "    \n",
    "    # init variables \n",
    "    filename = path+dbase+'_'+'log'+'.pkl'\n",
    "    dfList = []\n",
    "    \n",
    "    # get log data frame\n",
    "    for runNum in runList:\n",
    "        dfList.append(logFileDataFrame(cursor, dbase, runNum, path))\n",
    "        \n",
    "    # make log file from list of dataframes\n",
    "    dfLog = pd.concat(dfList, axis=0, ignore_index=True)\n",
    "    \n",
    "    # only if we have not done so already\n",
    "    if forceWrite or not os.path.isfile(filename):\n",
    "        dfLog.to_pickle(filename)\n",
    "        \n",
    "def pickleOneSensorFiles(cursor, dbase, run, eventId, path):\n",
    "    \n",
    "    log = pd.read_pickle(path+dbase+'_log.pkl')\n",
    "    \n",
    "    # find the last event id stored to pickle\n",
    "    s = str(log[(log.eventId==eventId) & (log.state =='1')].date.values[0])    \n",
    "    e = str(log[(log.eventId==eventId) & (log.state =='0')].date.values[0])    \n",
    "    pickleEvent(dbase, run, s, e, eventId, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pickleCrossDbaseSensorFiles(cursor, dbase, dbase_alt, run, run_alt, path, forceWrite = False):\n",
    "    print path+dbase+'_log.pkl'\n",
    "    log = pd.read_pickle(path+dbase+'_log.pkl')\n",
    "    log = log[log['run']==run]\n",
    "    \n",
    "    # find the last event id stored to pickle\n",
    "    dirList = os.listdir(path)\n",
    "    pklFileList = [s for s in dirList if 'pkl' in s and dbase+'_'+run in s  and dbase_alt+'_'+run_alt in s and not '_log.pkl' in s]\n",
    "    \n",
    "    temp = [x.split('_')[-1] for x in pklFileList] \n",
    "    \n",
    "    storedEvent = 0\n",
    "    if temp:\n",
    "        storedEvent = max([int(x.split('.')[0]) for x in temp])\n",
    "  \n",
    "    for eventId in log.eventId.unique():\n",
    "        if forceWrite or (eventId>storedEvent):\n",
    "            s = str(log[(log.eventId==eventId) & (log.state =='1')].date.values[0])    \n",
    "            e = str(log[(log.eventId==eventId) & (log.state =='0')].date.values[0])    \n",
    "            pickleCrossEvent(dbase, dbase_alt, run, run_alt, s, e, eventId, path)\n",
    "            \n",
    "def pickleCrossEvent(dbase, dbase_alt, run, run_alt, start, end, event, path):\n",
    "    cursor = connectDbase(dbase_alt)\n",
    "\n",
    "    # get table names, remove non data tables\n",
    "#     tbNameList = tableName(run)[1:-1]\n",
    "#     if dbase == 'rct008':\n",
    "#         tbNameList = tableName(run)[3:-1]\n",
    "    tbNameList = [run_alt+'_plcdata']\n",
    "\n",
    "    for tbName in tbNameList:\n",
    "        # read table\n",
    "        dfTable = readDateTable(cursor, tbName, start, end)\n",
    "        \n",
    "        # if it is not empty make a pickel file for each sensor\n",
    "        if len(dfTable) != 0:\n",
    "            # get rid of date\n",
    "            dfTable.drop('date', axis=1, inplace=True)\n",
    "            colList = [col for col in dfTable.columns if 'rtd' in col]\n",
    "            for col in colList:\n",
    "                filename = path+dbase+'_'+run+'_'+dbase_alt+'_'+run_alt+'_'+col+'_'+str(event)+'.pkl'\n",
    "                if len(dfTable[col]) !=0:\n",
    "                    print 'Pickling %s \\n'%filename\n",
    "                    pd.DataFrame(dfTable[col]).to_pickle(filename)\n",
    "                else:\n",
    "                    print 'Event: %d Run: %s Sensor: %s is null'%(event, run, col)            \n",
    "        else:\n",
    "            #if it's empty print this an keep going\n",
    "            print 'Table: %s is empty.\\n'%tbName"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
